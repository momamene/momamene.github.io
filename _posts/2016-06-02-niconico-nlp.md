---
published: true
title: '[번역] 니코니코동화의 공개코멘트 데이터를 Deep Learning로 해석하기'
layout: post
---
한자에 취약해서 사전을 찾아가면서 읽는데 따로 읽으나 번역하며 읽으나 속도에 큰 차이가 없어서 (-_-;) 이를 번역하여 공유합니다. 덕질로 습득한 일본어라 오역이나 의역이 있을 수 있고 이 부분에 대해서 양해를 드리며 댓글로 많은 지적 부탁드립니다.  
이 글은 qiita에 제2 드왕고(niconico를 서비스하고 있는 회사) Advent Calendar 2015의 24일째의 글을 번역한 글입니다. 원문은 [여기](http://qiita.com/ixixi/items/a3d56b2db6e09249a519)에서 볼 수 있습니다.  
좋은 글을 번역할 수 있게 허락해 주신 [@ixixi](https://twitter.com/ixixi)님에게도 감사의 말씀을 전합니다.  

개인적으로도 자연어 처리를 공부하고 있는 입장이라 댓글이나 SNS으로 같이 교류할 수 있으면 좋겠습니다.

# nico-opendata

niconico의 학술목적용 데이터 사이트 <https://nico-opendata.jp> 가 최근 오픈했습니다.  

이전에도 국립 정보학 연구소에서 니코니코동화 코멘트 데이터나 대백과 사전 데이터가 공개되어 있지만 nico-opendata에는 니코니코 이미지의 일러스트 데이터의 약 40만장의 일러스트와 메타데이터가 연구자들에게 제공되어 있습니다.  
이번에는 어디에서나 습득가능한 니코니코동화 코멘트 데이터을 이용하여 Deep Learning에 따른 코멘트를 분석한 사례를 소개합니다.

# 초자연언어

니코니코의 코멘트 데이터에 한정하지 않는, twitter의 tweet 등, Web 에서 사용하는 대화는 굉장히 자유로운 표현이 행해지고 있으며, 이러한 것이 문자의 해석을 방해하는 과제가 되어 있습니다.  
통상의 문장과 비교하여, 예를들어, 다음의 표현은, 의미의 해석이 어렵습니다.

(다음 링크가 코멘트를 이해하는데 도움이 될 것 같습니다. : <http://blog.naver.com/bound_mst/150119467078>)

* 절규 표현 :「こなああああああゆきいいいいいい 」「きたああああ 」「SUGEEEEEEE」
* 얼굴글자 :「(´・ω・`)ｼｮﾎﾞｰﾝ 」「(ﾟ∀ﾟ)ﾗｳﾞｨ!! 」「人生じんせいｵﾜﾀ＼(^o^)／ 」「囧*」
* 회화체 표현 :「くっさ 」「〜じゃね?」「やっべぇ!」
* 문장어가 아닌 표현 :「マジキチwww 」「みえ 」「●REC 」「おまwww」
  www는 한글의 ㅋㅋㅋ와 같다고 생각하면 된다.
* 특수한 의성어・의태어 :「8888888」
* 약칭 :「ksk 」「wktk 」「NKT 」「NDK 」「乙おつ 」「今北いまきた産業さんぎょう 」「〜な希まれガス」
* 서비스 고유 표현 :「わこつ 」「こうこつ 」「うぽつ 」「えんちょつ 」「tmt 」「184 」「んc」
* 통상과는 다른 의미 :「※ 」「馬鹿ばかなの?死しぬの? 」「わろすわろす(「わろすわろす」と「わろす」ではニュアンスがかなり異ことなる)」
* 특수기호(원문 : 템플릿 문법) :「なんでや!●●関係かんけいないやろ! 」「あぁ^～●●が△△するんじゃぁ^～ 」「一体いったい何なん◯なんだ…」
* 글 끝에 의미를 추가함 :「なんで・・・(泣* 」「美味おいしい(白目しろめ）」
* 장식 :「こ れ は 酷ひど い(スペース区切くぎり) 」「＼中村屋なかむらや!／」

사전(Dictionary) 기반의 처리는 기본적으로 문장 안의 단어를 전부 사전에 있는 (이 외에는 전부 미지어로 간주함)것으로 하는게 전제되어 있기 때문에 위와 같은 코멘트는 원래 글자를 적절히 분해하는 것이 어렵다고 알려져 있습니다.  
특히 특수기호 표현이나 장식표현을 바르게 인식하여 다루는 것은 무척 어려운 일이며 정규화에도 한계가 있습니다.  
사전에 의지하지 않는 방법으로는 N-gram(어떤 문장 중, N개의 단어로 구성된 문자열의 조합이 어떤 정도로 출현하는가를 확률적으로 분석하는 언어 모델)이 있지만 예를 들어 스페이스(공백) 단락 장식이 포함된 코멘트에 대해 N-gram을 수행할 경우 전혀 다른 성질이 되어버립니다. 즉, 인간이 다루는 것과 같은 문법구조를 인식하여 구문를 해석한 뒤, 거기서부터 의미를 파악하는 방법과는 멀리 떨어져있습니다.

# LSTM을 이용한 코멘트 해석

이와같은 붕괴된 글에 대하여, 어떤 방법으로 학습하는 것이 좋을까요?  
문자 전체에서 거시적 특징이 아닌, 서두에서부터 문자단위를 읽어가며, 계열정보를 사용하는 모델에서 코멘트의 의미를 파악하게 하고 싶다고 생각합니다.  
이를 위하여 LSTM이라고 불리는 계열정보를 학습가능한 모델을 사용하여 학습을 실행합니다.  
LSTM에 대해서는「[알려줘 LSTM ～ 최신 동향과 함께](http://qiita.com/t_Signull/items/21b82be280b46f467d1b)」의 기사가 굉장히 상세하게 되어 있으므로 이를 추천합니다.

## 태스크1 : 코멘트 다음 문자 예측

코멘트의 의미가 해석되어 있다면 중간부터 잘린 코멘트가 주어졌을 때 그 이후 이어지는 문자를 어떤 정도 예측할 수 있을것입니다.  
이것이 실제로 될 것인가 실험해 봅시다.  
즉,「처음부터 n글자까지 주어졌을 때, n+1글자를 예측하는」것이 테스크입니다.  
사전은 일절 쓰지 않습니다. 캐릭터(글자) 베이스로 진행합니다.  

### 데이터셋

[국립정보학연구소의 정보학연구 데이터 리포지토리](http://www.nii.ac.jp/dsc/idr/nico/nico.html)에서 공개되어 있는 코멘트를 이용합니다.  

### 사전작업

공개되어 있는 24억 코멘트를 전부 학습시키는 필요는 없다고 생각했기 때문에, 학습할 코멘트를 이하와 같이 추출했습니다.

1. 각 동영상의 최후의 10코멘트를 추출
2. 그 안에서, 500만 코멘트를 랜덤 샘플링
3. 코멘트를 NFKC 정규화하여 끝의 동일 문자 반복을 2글자로 줄인다. (「테라와로스 wwwwww」→「테라와로스 ww」) 
4. 해당 코멘트에 나타나는 문자를 카운트하여 상위 5000문자를 어휘로 사용함
5. 상위 5000문자 이외의 문자를 합친 코멘트는 무시(거의 없음)

1에 대해서는 각 동영상 최초 10코멘트가 아닌, 최후의 10코멘트를 사용하는 것은, 최초의 10코멘트는「댓글 1등」이나「업로드 ㅅㄱ」(원문 :「1ゲット」,「うp乙」)와 같은 특정 코멘트가 들어있는 경향이 있기 때문입니다. 또한, 순수한 랜덤 샘플링에서 코멘트를 수집하면 코멘트의 양이 방대한 특정한 동영상에 학습이 편중되기 때문에 니코니코 안에서 일반적인 어휘를 학습하기에는 맞지 않는다고 판단했기 때문입니다. (더욱이, 코멘트 랭킹 1위의 동영상의 코멘트의 갯수는 3000만 코멘트 이상입니다.)

3의 반복된 글자에 대한 정규화는 다음 문자를 예측할 경우「"w"가 올 때 다음의 글자도 "w"이다.」라고 예측하는 것만으로도 손쉽게 정답률을 올릴 수 있기 때문입니다.

### 결과

GPU 머신으로 수 일동안 돌려서 다음과 같은 결과가 있었습니다. ( 모델의 미묘한 파라미터를 선의 색으로 구분하였습니다. ) 

![](https://qiita-image-store.s3.amazonaws.com/0/8954/3f9933cb-4d58-4aaa-09c5-fefc082004f3.png)

이후의 글자를 40%정도로 맞출 수 있었습니다.  
이것은,「아무것도 아닌 상태에서 1글자」의 예측과「코멘트 종료」를 포함한 결과입니다.  
(참고로, 조금 전의 반복된 글자에 대한 정규화를 실행한 경우, 정답률은 50%으로 올라갑니다.)

### 코멘트 자동생성

다음 글자의 예측이 가능하다는 것은 이것을 이용하여 미완성의 코멘트가 주어졌을 때 이후의 코멘트를 만드는 것이 가능할 것입니다.

실제로 해 보았습니다.

|인력|자동생성|
|----|----------|
|┗(^ | ┗(^o^ )┓三|
|日本語 | 日本語でおk|
|/hi | /hidden|
|おっく| おっくせんまん！おっくせんまん！|
|らんら | らんらんるー|
|ξ*・ | ξ*・ヮ・*|
|わっふ | わっふるわっふる|
|かわい | かわいいww|

공백 표현이나 괄호 대응도 제대로 학습되어 있다는 재미있는 결과도 얻을 수 있었습니다.

|인력|자동생성|
|----|----------|
|「は |「はぁ?」|
|こ れ は |こ れ は ひ ど い|
|(((( | (((((* ´ Д ｀)))))(´Д｀)( * ´д｀*)|
|犯人は | 犯人はヤス|

직전의 문자뿐만이 아니라 의미에 맞게 생성하고 있음을 확인하기 위해「〜の」( 역주 : 한국어로 하면「~의」의 의미와 비슷하다. ) 부터 시작하는 코멘트를 추출하여「〜の」이후를 생성하게 해 보았습니다.

|인력 | 자동생성   |
|---------------------|----------------------------------------------------------|
|この（이 )           | この曲好きだ ( 이 음악 좋다 )                            |
|あの ( 저; 그. )     | あのさぁ・・ ( 저기… )                                   |
|なんだこの (뭐지 이) | なんだこの画質 ( 뭐지 이 영상 )                          |
|今の (지금의)        | 今のはなんだったんだw (지금 뭐였던거지ㅋ)                |
|ミクの (미쿠의)      | ミクの声が聴こえる (미쿠의 목소리가 들린다)              |
|謎の (수수깨끼의)    | 謎の感動 (수수깨끼의 감동)                               |
|ゲームの (게임의)    | ゲームの音が聞こえない (게임의 소리가 들리지않아)        |
|他の (다른)          | 他の人の動画でもみたいな (다른 사람의 동영상에서도 같은) |
|うp主の (업로더의)   |うp主の声好きだなぁ (업로더의 목소리 좋아)                |

그럭저럭「の」의 앞까지의 글자의 의미가 제대로 전해져 있습니다. 

각「~の」에 대하여 출력된 출력층(5002차원)의 값을 PCA로 2차원으로 떨어뜨려 확대해 보면, 가령 이하와 같은 클러스터가 존재하고 있습니다.

「中国の (중국의)」「日本の (일본의)」와 같이「(国)の ((나라)의)」에 속하는 개념이 붙어 있다거나「当時の (당시의)」「今日の (오늘의)」「今年の (올해의)」「次の (다음의)」과 같은「(時間)の ((시간)의)」의 개념이 뭉쳐져 있습니다.

![](https://qiita-image-store.s3.amazonaws.com/0/8954/a43633c8-2a9f-376a-50f0-1788de8c2bd5.png)

또한,「(体の一部)の ((몸의 일부)의)」나「(音/声)の (소리/목소리)의 」같은것도 다음의 이어지는 문자가 가까이 있다는 것을 알 수 있습니다.

![](https://qiita-image-store.s3.amazonaws.com/0/8954/e843bf9f-e0c8-4a76-b5af-8b3acfefdf58.png)


## 태스크2 : 코멘트로부터 동영상의 카테고리 예측

현재 niconico의 영상은 30카테고리가 있습니다.  
( 음악 / 불러 보았다 / 게임 / 애니메이션 / 보컬로이드 / 동방 / 기타 / 엔터테인먼트 / 연주해 보앗다 / 라디오 / 아이돌마스터 / 니코니코인디즈 / 스포츠 / 그려보았다 / 니코니코 동화 강좌 / 요리 / R-18 / 동물 / 정치 / 과학 / 일기 / 저번의 그것 / 니코니코 기술부 / 여행 / 자연 / 만들어 보았다 / 역사 / 춤춰 보았다 / 니코니코 수예부 / 도로주행 영상 )  
태스크는「1코멘트 등록 시 그 코멘트는 어떠한 카테고리의 동영상일까」를 맞추는 작업입니다.

### 학습 세트 예시

이하와 같은 데이터세트에 대하여 본문 괄호 안에는 정답 라벨이 있습니다.  
「ww」와 같은 코멘트의 카테고리를 식별하는 것은 인간도 상당한 난이도의 작업입니다.  
상당히 숙련된 인간이라도 힘껏 20%정도만 정답 수 없는 작업이 아닌가 싶습니다. (필자는 15%정도의 정답율이었습니다.)

### 어프로치

태스크1에서 다음 문자 예측을 위해 생성한 모델은 그대로는 카테고리 예측으로는 사용하지 못하지만 내부는 문법구조나 어휘의 학습이 되어있을 것입니다. 그렇다면 이것을 이용하여 이 모델을 카테고리 예측에도 사용할 수 있을 것이라 생각하였습니다. 실제로 그러한지 이를 검증하기 위하여 이하의 4개의 패턴의 실험을 진행해 보았습니다.

1. 태스크 1의 다음 문자 예측 모델과 같은 구조로 처음부터 학습하는 패턴
2. 다음 문자 예측 학습 후 모델의 출력층만 달고 학습하는 패턴
3. 다음 문자 예측 모델에 층을 추가한 구조로 처음부터 학습하는 패턴
4. 다음 문자 예측 학습 후 모델에 Layer를 추가하고 학습한 패턴

### 결과
몇가지를 실험해 보았으나 결과는「다음 문자 예측 학습 후 모델에 Layer를 추가하고 학습한 패턴」이 가장 정확도가 높고 18%정도의 정답률이 나오게 되었습니다.

![](https://qiita-image-store.s3.amazonaws.com/0/8954/3a578c95-cda9-a7de-3b73-32ff44c4af70.png)


### 시각화

출력층에서 나온 출력(30차원)을 t-SNE로 가시화 해 보면 이하와 같이 나타납니다.  
각 코멘트의 삭이 그 코멘트가 달린 동영상의 카테고리입니다.  
(보기 편하도록 9카테고리의 코멘트가 보이도록 하였습니다.)  
대강 보기에도 같은 색이 모여 있는 것으로 보입니다.  

![](https://qiita-image-store.s3.amazonaws.com/0/8954/efd5ed51-1731-9da3-10ac-0643a410bb71.png)

「동물」계 코멘트

![](https://qiita-image-store.s3.amazonaws.com/0/8954/b9087044-8339-d946-b8be-024324f1195c.png)

「요리」계 코멘트

![](https://qiita-image-store.s3.amazonaws.com/0/8954/c7dedc27-e064-e5e0-6a1c-4444b28e5b4a.png)

「스포츠」계 코멘트

![](https://qiita-image-store.s3.amazonaws.com/0/8954/f96fc0d7-70bf-a319-f51f-2248fc9958fd.png)


「도로주행 영상」계 코멘트

![](https://qiita-image-store.s3.amazonaws.com/0/8954/611df9ec-574a-5fd1-9ff6-3d5dafc9f77c.png)

「연주해 보았다」계 코멘트

![](https://qiita-image-store.s3.amazonaws.com/0/8954/ab22bb71-c4ae-a54d-c5b2-b194b6acbe2c.png)

그럭저럭 의미가(카테고리 분류에 필요한 수준으로)잘 전달되는 것 같습니다.

## 유사 코멘트 제안

위에서 학습한 코멘트 카테고리 식별기는 출력층의 하나 전의 Layer를 제거하면 (카테고리 식별이 필요한)코멘트의 의미를 표현하는 특징(Feature)이 되는 것으로도 해석할 수 있습니다.  
그렇다는 것은 근방 탐색을 실행하는 것으로 의미적으로 유사한 코멘트를 찾는 것이 가능하고 코멘트와 코멘트 사이의 의미적인 거리도 정의할수 있을 것 같습니다.

### 아키텍처

![](https://qiita-image-store.s3.amazonaws.com/0/8954/e22291c5-8ba8-af91-62e4-8f4848794ad4.png) 

이 구조는 간단히 어플리케이션으로 할 수 있고 [Jubatus](http://jubat.us/ja/)라는 온라인 분류기를 사용하면 코멘트의 특징량(Feature)을 ID를 붙여 던져 주면 LSH나 minhash와 같은 근사 최근 주변 탐색으로 넣어 둔 코멘트 중 특징량이 가까운 코멘트를 얻을 수 있습니다. [Jubarecommender](http://jubat.us/ja/api_recommender.html)를 사용했습니다.

![](https://qiita-image-store.s3.amazonaws.com/0/8954/e22291c5-8ba8-af91-62e4-8f4848794ad4.png)

이번에는 사전에 일괄로 니코니코 데이터세트의 코멘트 데이터의 특징량을 Jubatus에 저장하겠습니다. 이번에는 하진 않지만 이 작업으로 Jubatus도 실시간으로 후보를 추가할 수 있고 Chainer도 온라인으로 추가로 학습할 수 있기 때문에 실시간 학습이라는 면에서 재미있을지도 모릅니다. (학습률 조정을 어떻게 할지에 대한 과제는 있을 것 같습니다만)

### 코멘트 제안 예시

위의 하얀 직접 입력한 폼이 인력 코멘트, 아래의 「近傍投稿」가 유사한 코멘트 입니다.

![](https://qiita-image-store.s3.amazonaws.com/0/8954/f514d561-322a-03bb-55a8-b201267c4b0c.jpeg)

 「可愛いなぁ」의 입력에 대하여 「かーわーいーいー」나 「かぁわいい」「かわいい^^」「かわいすぎるううううううう>w<」「かわゆす」등과 같이 비슷한 특징을 가진 코멘트가 생성되었습니다.

![](https://qiita-image-store.s3.amazonaws.com/0/8954/6ef62f13-ecc8-68ff-83ad-3441594be433.jpeg)

「お疲つかれ様さま(수고하셨습니다)」과 「乙おつ」가 동일한 특징을 가진 것을 알 수 있습니다.

![](https://qiita-image-store.s3.amazonaws.com/0/8954/2a3d4c77-c331-0eb4-7b99-779dcf47e5ea.jpeg)

「どうしてこうなった(어쩌다 이렇게 됐지)」≒「!?」

![](https://qiita-image-store.s3.amazonaws.com/0/8954/df937bb4-67e3-0bb0-ca24-b85028819e6b.jpeg)

「勉強になるなぁ(공부가 되는군)」≒「ほぅ」「へ〜」「fmfm」

## 니코니코의 코멘트 해석 요약

LSTM의 케릭터 데이스의 접근은 사전을 일절 사용하지 않고 카테고리 라벨을 교사 데이터로 한것만으로 코멘트의 의미를 어느정도 학습할 수 있고 식별도 유사 검색도 적용 가능하다는 것도 알았습니다.  
카테고리 태그는 겨우 30개밖에 없지만 주요 태그 라벨로 학습하게 한다면 더욱 세세한 의미의 학습이 가능한 것 같습니다.  
특히, 유사 코멘트의 제안에 대하여 「お疲つかれ様さま」와 「乙」, 「勉強になるな」와 「fmfm」와 같이 이용하고 있는 문자도 전혀 다른 표면적으로 전혀 다른 코멘트의 대하여, 의미적으로 가까운 것을 제안할 수 있는 것은 재미있는 결과라고 생각합니다.

# 보다 섬세한 코멘트 해석을 향하여

이하 기재한 내용은 아직 실험중이지만 더욱 세세한 의미를 학습하는 접근 방식에 대하여 설명합니다.

## 올라온 코멘트 특징의 동영상 안에서의 국소성

이전에 카테고리가 아닌 태그를 사용하면 더욱 미세한 의미의 학습이 가능하지 않을까 적었었지만 더 말하면 태그화되지 않은 수준의 구체적인 정보가 있을 것입니다.  
한 영상의 안에서도 씬에 따라 갖고 있는 컨텍스트가 다른 경우 달려 있는 코멘트의 특징이 다를 것입니다.  
「코멘트 특징의 영상 안의 국소성이 존재함(같은 내용의 코멘트는 같은 장소에 달림) 」라는 것은 자연히 가정할 수 있을 것 같습니다.  

다음의 영상에서는 「かわいい」라는 의미의 말이 국소적으로 발생하고 있는 것을 알 수 있습니다.  

![](https://qiita-image-store.s3.amazonaws.com/0/8954/28d214b3-119e-f4a5-3f11-70bc448ccc08.png)

## 동영상 안 코멘트 위치에서 코멘트 특징 학습

이 국소성으로 부터 코멘트의 의미를 학습하는 것을 생각하였습니다. 「의미적 거리가 그대로 특징량 사이의 거리가 되는 특징량」을 출력하는 네트워크를 목표로 하고 있습니다.  
F(c_1)은 이 네트워크에 코멘트 c_1 을 넣었을 때의 k 차원 벡터(코멘트는 의미를 표현한 특징량)을 표현한 것입니다.  
2개의 코멘트(c_1, c_2)가 있을 때, 이 의미의 가까운 정도가 특징량 벡터의 거리 distance(F(c_1), F(c_2)) 로 표현된다면 이 특징량 공간은 근방 탐색 등에서 유용한 특징량이 추출될 수 있다고 생각합니다.  
출력되는 특징량으로서는, 이하의 성질을 기대합니다.

1. c_1과 c_2가 동일한 time window 안의 코멘트에 있다면(한 화면에 코멘트가 같이 나오는 것을 의미하는 것 같습니다.) 의미적으로 동일한 코멘트이기 때문에 코멘트 특징량 사이의 거리는 가까이 되고 싶습니다.
2. c_1과 c_2가 동일한 영상에서 time window 밖의 코멘트에 있다면 코멘트 특징량 사이 거리는 어느정도 (α 이상) 떨어져 있게 하고 싶습니다.
3. c_1과 c_2가 다른 동영상의 코멘트라면 코멘트 특징량 사이 거리는 어느정도(β이상) 떨어져 있고 싶습니다.
4. 0 < α < β

이것을 순수하게 loss function 으로 구현하면 다음과 같습니다.

1. c_1와 c_2는 같은 영상에 있고 c_1의 영상 안의 위치 주변에 c_2가 있다면 `distance(F(c_1), F(c_2))`가 loss.
2. c_1과 c_2가 같은 영상이지만 c_1의 영상 안 위치 주변에 c_2가 없다면 `max(α-(F(c_1),F(c_2),0)`가 loss.
3. c_1과 c_2가 다른 영상이라면 max(β-(F(c_1),F(c_2),0)가 loss.

loss function으로는 chainer v1.5부터 추가된 [contrasive loss](http://chainer.readthedocs.org/en/stable/reference/functions.html?highlight=contrastive#chainer.functions.contrastive)를 사용할 수 있을 것 같습니다.(이 Function을 사용하지 않아도 Variable로 공통의 처리는 사용할 수 있지만)  
이 방법으로 실험 결과가 나오면 다시 어딘가에서 기사를 쓰려고 합니다.

## [덤] contrative loss와 상대적 관계의 학습에 대하여(일반적인 이야기)

contrastive loss는 y_n이 1(동일한 클래스)이라면 벡터간의 거리가 loss가 되며 0(다른 클래스)라면 지정한 margin보다 가까웠던 만큼의 거리가 loss(margin 이상 떨어진다면 loss가 0)입니다. 즉, 「동일한 클래스라면 가능한 만큼 뭉치고 다른 클래스라면 margin 이상 떨어진다 」의 방식으로 최적화 됩니다.
이 loss function은 다음과 같이 되어 있습니다.

<DIV>
$$ L=\frac{1}{2N}\left(\sum_{n=1})N y_n d_n^2+(1-y_n)\max({\rm margin}-d_n, 0)^2\right) $$
</DIV>

일반적으로 N클래스 분류의 문제는 출력층을 N차원으로 하여 이것에 [softmax cross entropy](http://chainer.readthedocs.org/en/stable/reference/functions.html#chainer.functions.softmax_cross_entropy)를 적용하여 입력에 대하여 「어느 클래스에 가장 가까운가」를 예측하는 방식으로 학습합니다. 이것은 처음 클래스의 갯수를 알고 있음을 전제하는 모델입니다.  
상대적 관계의 학습은 클래스의 수는 관계없습니다. 「(이미지A, 이미지B)는 동일한 종류」「(이미지C, 이미지D)는 다른 종류」와 같은 형태의 상대적인 관계의 정보만으로 학습을 진행하기 때문에 최초에 전체의 클래스의 수를 알지 못하는 경우에도 대응할수 있어야 합니다.

그런데 일반적인 N클래스 식별에서 특징을 추출하는 경우 이 N차원의 출력 혹은 이전 Layer의 특징량으로 하는 경우가 많습니다. 예를 들어 이미지 분류에서 잘 사용되는 VGG모델은 출력층 직전은 4096차원의 벡터로 되고 이것을 특징량으로 하는 것이 많은 것 같습니다.  
단지 [4096차원을 t-SNE로 2차원으로 떨어트려 시각화하는 것](http://cs.stanford.edu/people/karpathy/cnnembed/)을 봐도 꽤 깨끗하게 클래스가 떨어져있고 저차원에도 많은 차원의 특징을 표현하는 것이 충분히 가능하다고 느낍니다.  
따라서 특징량을 출력층이 저차원으로 하여 이 공간에도 거리를 학습해 보도록 하겠습니다.

### 사례 1 : MINST에서의 예

네트워크의 최후 출력층을 2차원으로 하여 「클래스가 동일한 것을 입력한 경우 출력층의 2차원 벡터의 거리가 최대한 가까이 되도록 다시 클래스가 다른 것을 입력한 경우 거리가 1이상 다른 2차원 벡터가 출력됨 」으로 학습하면 출력된 벡터는 다음과 같이 됩니다.(2차원의 출력이기 때문에 PCA나 t-SNE는 사용하지 않고 출력 그 자체를 표현한 것입니다.)

![](https://qiita-image-store.s3.amazonaws.com/0/8954/5756a021-dc4b-3a78-d3e8-b6d6b6000780.png)

클래스의 라벨을 붙이지 않고 붙여진 쌍이 동일한지 다른지에 대한 정보만으로 예쁜 클래스 식별이 되며 각 클래스 사이의 거리는 10(클래스 간의 margin으로 설정한 값)이 되는 것을 알 수 있습니다.

### 사례 2 : CIFAT-10에서의 예

MNIST와 함께 단골로 사용되는 [CIFAT-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)에서도 해 봅시다.  
이것은 10종류(airplane/automobile/bird/cat/deer/dog/frog/horse/ship/truck)의 라벨이 붙여져 있는 6000장의 사진 데이터셋입니다.

![](https://qiita-image-store.s3.amazonaws.com/0/8954/83650abc-265c-11a3-1d94-7325201ca0dd.png)

클래스(airplane/automobile/...)보다 넓은 카테고리(동물/탈것)이 에 대해서도 똑같이 contrastive loss를 적용하여 클래스의 contrastive loss와 합하여 optimize하기 때문에 카테고리도 분리됩니다.

![](https://qiita-image-store.s3.amazonaws.com/0/8954/eb71586c-bbf2-ce92-eb36-01461c3e4eda.png)

계층적인 관계도 포함하여 상대적인 정보로부터 좋은 특징 공간을 만들었습니다.

이 방법을 사용하는 경우 단순히 랜덤하게 짝을 모으면 짝 그룹의 다른 클래스가 되어 버리기 때문에 여기서는 각각 균형있게 학습 데이터의 짝을 만들 필요가 있습니다.

# 마무리하며

크리스마스, 크리스마스, 크리스, 마스.

[Steins;Gate](http://steinsgate.jp/)의 「크리스」가 떠올리는 분도 계시지 않을까요?
드왕고에서는 "크리스"라는 이름의 수십대의 [Titan X](http://www.nvidia.co.jp/object/geforce-gtx-titan-x-jp.html)4장 꽂은 노드들이 있습니다.(안의 2대는 1TB의 메모리)  
드왕고에서는 이 크리스의 환경을 코멘트 해석 뿐만아니라 일러스트 해석 등에 사용하고 있습니다.  
예를들어, <https://nico-opendata.jp/>에 개제하고 있는 [이미지에서 조회 수 ・좋아요 수 예측](https://nico-opendata.jp/ja/casestudy/view_count_prediction/index.html)의 실험은 크리스 위에서 실행된 것입니다.  
코멘트의 해석에 관해서는 올해 Chainer에서 LSTM의 모델의 코멘트 해석 엔진이 niconico의 서비스의 일부에서 실제 도입되고 있습니다.(실제 환경은 Tesla GPU머신입니다.)  

올해는 Deep Learning의 해였다고 불릴 정도로 이 단어를 많이 듣는 해였던것 같습니다.  
「이후의 시스템 내장 및 운용을 생각해 보면 Deep Learning이외의 방법이 좋지 않은가?」와 생각되는 경우도 실제로는 많지만 적합한 부분에 사용되면 역시 강렬한 효과가 분명히 나옵니다.

「[기계학습의 기술적부채](http://research.google.com/pubs/pub43146.html)」는 매우 크기 때문에 적절한 목적을 설정하여, 업무에 맞는 적절한 알고리즘을 골라, 적절한 모델링을 하여, 적절한 전처리를 하여, 적절한 시스템의 갖추고, 적절한 운용하는것(무척 어렵지요...)을 강하게 의식하여 이제부터의 서비스의 적용을 생각하고 싶다고 골똘히 생각한 한 해였습니다.
